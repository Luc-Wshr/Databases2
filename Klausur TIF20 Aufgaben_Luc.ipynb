{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41064b1f",
   "metadata": {},
   "source": [
    "Aufgabe 1: [15/120 Punkte] Nenne 5 **verschiedene** Anwendungen wo Key-Value-Datenbanken Vorteile gegenüber Relationalen Datenbanken haben, und beschreibe spezifisch (aber dennoch stichwortartig) für jeden Fall, was genau der jeweilige Vorteil ist (und: was potentielle Nachteile sind)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a77844b-04cc-48ca-8932-9a869ab7a15b",
   "metadata": {},
   "source": [
    "1. Online Warenkörbe wie bei Amazon. Da Key-Value Datenbanken schnelle kurze Abfragen machen können und relationale Datenbanken mit Joins arbeiten müssten welche teuer sind\n",
    "2. Anwendungen bei welchen keine häufigen Updates gemacht werden müssen, da diese bei relationalen Datenbanken teuer sind und bei key-value datenbank schnell durchgeführt werden\n",
    "3. Beim Caching sind Key-Value Datenbanken besser geignet als Relationale Datenbanken, da es hier um schnelle kurze Abfragen geht\n",
    "4. Beim Gaming sind Key-Value Datenbanken besser geignet, da die notwendigen Daten schnell und in realtime verarbeitet werden können\n",
    "5. In Social Media Systeme sind Key-Value Datenbanken besser geigenet, da diese schnelle Transaktionen durchführen können"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa609eeb",
   "metadata": {},
   "source": [
    "Aufgabe 2: [15/120 Punkte] erkläre was denn nun wirklich die Hauptunterschiede zwischen einem Data Warehouse und einem Data Lake sind und gib Beispiele für Anwendungen in denen entweder ein Data Warehouse oder ein Data Lake die bessere Wahl ist"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dc350ef-5470-4728-8efc-84d13d11cc50",
   "metadata": {},
   "source": [
    "In einem Data Lake können strukturierte und unstrukturierte Daten gespeichert werden und in einem Data Warehouse nur strukturierte Daten\n",
    "Bei einem Data Warehouse ist schon klar wofür die Daten verwendet werden und bei einem Data Lake noch nicht (diese werden dort einfach gesammelt)\n",
    "Beim Data Warehouse sind die Struktur und Schema der Daten vorgegeben\n",
    "Ein Data Lake kann die bestehendes Lagecy System optimieren und effizienter machen\n",
    "\n",
    "Anwendung für Data Warehouse:\n",
    "    Analysesysteme um Daten zu Analysiern\n",
    "    Systeme zur Verwaltung von kundendaten\n",
    "    Marketingsysteme\n",
    "    Erstellung von Berichten\n",
    "Anwendung für Data Lake:\n",
    "    Transaktionssyseme\n",
    "    Big Data Analyse Systeme\n",
    "    Supplychain Optimierungssysteme\n",
    "    Speichern von Daten aus verschiednene Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a4491",
   "metadata": {},
   "source": [
    "Aufgabe 3: [15/120 Punkte] im folgenden Code hat sich der Fehlerteufel eingeschlichen. Finde die 5 Fehler (10 Punkte), und beschreibe stichwortartig, was der Code denn tun würde, wenn keine Fehler drin wären. (5 Punkte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334810e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'[' was never closed (2221102690.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[79], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    pipe2 = [ { '$mismatch' : { 'origin' : 'ATL',\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '[' was never closed\n"
     ]
    }
   ],
   "source": [
    "from bson.son import SON\n",
    "\n",
    "# MongoClient Import fehlt, wird hier nachfolgend aufgerufen, wurde aber nie importiert\n",
    "client = MongoClient(\"localhost\", 27017)\n",
    "\n",
    "# Die Syntax für die Connection zur Datenbank & Collection ist nicht korrekt, muss separat gemacht werden, und nicht in einem Schritt zusammen\n",
    "connect = client[\"database\"],[\"collection\"]\n",
    "\n",
    "# $mismatch existiert nicht, $match sollte verwendet werden\n",
    "pipe2 = [ { '$mismatch' : { 'origin' : 'ATL',\n",
    "                         'dest' : 'BOS',\n",
    "                         'dayofweek' : 3\n",
    "                       }\n",
    "          },\n",
    "          { '$group' : { '_id' : { 'origin' : '$origin',\n",
    "                                   'destination' : '$dest'\n",
    "                                 },\n",
    "                         'Failure' : { '$sum' : { '$cond' : [{ '$eq' : ['$cancelled', 1]}, 1, 0 ]} },\n",
    "                         'Success' : { '$sum' : { '$cond' : [{ '$eq' : ['$cancelled', 0]}, 1, 0 ]} },\n",
    "                         'Total' : { '$sum' : 1 }\n",
    "                        }\n",
    "           },\n",
    "           { '$project' : { 'Failure' : 1,\n",
    "                            'Success' : 1,\n",
    "                            'Total' : 1,\n",
    "                            'FailPercent' : { '$divide' : [ '$Failure', '$Total' ] }\n",
    "                          }\n",
    "           },\n",
    "           # im SON-Statement ist kein Fehler, das gehoert so \n",
    "           { '$sort' : SON([('_id.origin', 1), ('_id.destination', 1 )]) }\n",
    "         \n",
    "# Hier fehlt eine schließende Klammer ] für die Syntax der Pipeline, um dieser zu sagen dass die Pipeline nun zu Ende ist.\n",
    "         \n",
    "# Hier bei result = wird eine Variable \"connection\" aufgerufen, die vorher nicht definiert wurde, die Variable, die hier benutzt werden muss, ist \"connect\" wie oben definiert worden ist\n",
    "result = connection.aggregate(pipeline=pipe2)\n",
    "print(result)\n",
    "         \n",
    "         \n",
    "# Wenn keine Fehler in dem Code drin wären, würde es zuerst eine Verbindung zu einer lokalen MongoDB (localhost) auf port 27017 herstellen, dann eine Verbindung zu der Datenbank \"Database\",\n",
    "# und zur Collection \"collection\" aufgebaut. Danach wird eine Pipeline erstellt wo ein match, ein group, ein project und ein sort definiert werden, welche dann zum Schluss in unserer Collection abgerufen wird. Zuletzt wird dann noch das ergebnis geprinted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae06e21",
   "metadata": {},
   "source": [
    "Aufgabe 4: [15/120 Punkte] Erkläre was lazy evaluation ist an Hand von 2 in den Vorlesungen behandelten Technologien/Ansätzen (1 Lazy, eine nicht lazy) - und erläutere, was die Auswirkungen in der Praxis sind."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4acda586-b967-4e0b-a00b-c7afa19d64d7",
   "metadata": {},
   "source": [
    "Lazy Evaluation ist die \"faule\" Abgfrage von Daten.\n",
    "Beispielsweise ist der \"find\" befehl Lazy, dieser ruft nur die Daten ab, wenn sie wirklich gebraucht werden.\n",
    "\n",
    "Eine nicht Lazy Evaluation frägt den gesamten Datensatz auf einem ab,\n",
    "das macht z.B. eine aggregation. Hier wird jeder Eintrag in eine Liste geschrieben und jedes mal um einen Eintrag erweitert, welche immer umfangreicher und größer wird.\n",
    "\n",
    "\n",
    "Die Auswirkungen in der Praxis:\n",
    "\n",
    "- Nicht Lazy Evaluation ist nicht besonders Performant\n",
    "- Lazy-Evaluation könnte dazu führen dass manche Daten noch nicht abgerufen wurden, und daher noch nicht genutzt werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322834c",
   "metadata": {},
   "source": [
    "Aufgabe 5: [40/120 Punkte]\n",
    "    In Moodle findest Du Super-Markt Daten. Werte diese in Spark aus, und finde heraus:\n",
    "    \n",
    "    1) wie viele Einkäufe wurden von Frauen, bzw von Männern getätigt?\n",
    "    2) wie viel Umsatz wurde an den 3 Standorten jeweils getätigt - gib diesen zusammen mit dem NAMEN des jeweiligen Standorts aus\n",
    "    3) welche Bezahlart die höchste durchschnittliche User-Satisfaction hat\n",
    "    \n",
    "Exportiere die Ergebnisse von Teilaufgabe 3 als CSV-Datei - stelle dabei sicher, dass unabhängig von der Größe des Datensatzes nur 1 einzige CSV-Datei generiert wird.\n",
    "\n",
    "Beschreibe für jede der Teilaufgaben wie Du die Daten dazu aufbereiten würdest (Stichwort: Designentscheidungen)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae5f73c-d839-4339-8221-3838c43c8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"Name der DB\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282a0eed-3a89-41c1-b348-387e687ae3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Name der DB</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6a44ff1ae0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb75d019-26cb-4cbb-85bb-bd40069b9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+-------------+------+----------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "|Invoice ID |Branch|City     |Customer type|Gender|Product line          |Unit price|Quantity|Tax 5% |Total   |Date     |Time |Payment    |cogs  |gross margin percentage|gross income|Rating|\n",
      "+-----------+------+---------+-------------+------+----------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "|750-67-8428|A     |Yangon   |Member       |Female|Health and beauty     |74.69     |7       |26.1415|548.9715|1/5/2019 |13:08|Ewallet    |522.83|4.761904762            |26.1415     |9.1   |\n",
      "|226-31-3081|C     |Naypyitaw|Normal       |f     |Electronic accessories|15.28     |5       |3.82   |80.22   |3/8/2019 |10:29|Cash       |76.4  |4.761904762            |3.82        |9.6   |\n",
      "|631-41-3108|A     |null     |Normal       |Male  |Home and lifestyle    |46.33     |7       |16.2155|340.5255|3/3/2019 |13:23|credit card|324.31|4.761904762            |16.2155     |7.4   |\n",
      "|123-19-1176|A     |Yangon   |Member       |male  |Health and beauty     |58.22     |8       |23288  |489048  |1/27/2019|20:33|Ewallet    |465.76|4.761904762            |23288       |8.4   |\n",
      "|373-73-7910|A     |Yangon   |Normal       |Male  |Sports and travel     |86.31     |7       |30.2085|634.3785|2/8/2019 |10:37|ewallet    |604.17|4.761904762            |30.2085     |5.3   |\n",
      "|699-14-3026|C     |Naypyitaw|Normal       |Male  |Electronic accessories|85.39     |7       |29.8865|627.6165|3/25/2019|18:30|Ewallet    |597.73|4.761904762            |29.8865     |4.1   |\n",
      "|355-53-5943|A     |Yangon   |Member       |f     |Electronic accessories|68.84     |6       |20652  |433692  |2/25/2019|14:36|Ewallet    |413.04|4.761904762            |20652       |5.8   |\n",
      "|315-22-5665|C     |null     |Normal       |Female|Home and lifestyle    |73.56     |10      |36.78  |772.38  |2/24/2019|11:38|Ewallet    |735.6 |4.761904762            |36.78       |8     |\n",
      "|665-32-9167|A     |Yangon   |Member       |f     |Health and beauty     |36.26     |2       |3626   |76146   |1/10/2019|17:15|Creditcard |72.52 |4.761904762            |3626        |7.2   |\n",
      "|692-92-5582|B     |Mandalay |Member       |Female|Food and beverages    |54.84     |3       |8226   |172746  |2/20/2019|13:27|creditcard |164.52|4.761904762            |8226        |5.9   |\n",
      "|351-62-0822|B     |Mandalay |Member       |Female|Fashion accessories   |14.48     |4       |2896   |60816   |2/6/2019 |18:07|Ewallet    |57.92 |4.761904762            |2896        |4.5   |\n",
      "|529-56-3974|B     |null     |Member       |m     |Electronic accessories|25.51     |4       |5102   |107142  |3/9/2019 |17:03|cash       |102.04|4.761904762            |5102        |6.8   |\n",
      "|365-64-0515|A     |Yangon   |Normal       |Female|Electronic accessories|46.95     |5       |11.7375|246.4875|2/12/2019|10:25|Ewallet    |234.75|4.761904762            |11.7375     |7.1   |\n",
      "|252-56-2699|A     |null     |Normal       |m     |Food and beverages    |43.19     |10      |21595  |453495  |2/7/2019 |16:48|Ewallet    |431.9 |4.761904762            |21595       |8.2   |\n",
      "|829-34-3910|A     |Yangon   |Normal       |Female|Health and beauty     |71.38     |10      |35.69  |749.49  |3/29/2019|19:21|Cash       |713.8 |4.761904762            |35.69       |5.7   |\n",
      "|299-46-1805|B     |Mandalay |Member       |Female|Sports and travel     |93.72     |6       |28116  |590436  |1/15/2019|16:19|cash       |562.32|4.761904762            |28116       |4.5   |\n",
      "|656-95-9349|A     |Yangon   |Member       |Female|Health and beauty     |68.93     |7       |24.1255|506.6355|3/11/2019|11:03|Creditcard |482.51|4.761904762            |24.1255     |4.6   |\n",
      "|765-26-6951|A     |null     |Normal       |Male  |Sports and travel     |72.61     |6       |21783  |457443  |1/1/2019 |10:39|Credit card|435.66|4.761904762            |21783       |6.9   |\n",
      "|329-62-1586|A     |Yangon   |Normal       |m     |Food and beverages    |54.67     |3       |8.2005 |172.2105|1/21/2019|18:00|Creditcard |164.01|4.761904762            |8.2005      |8.6   |\n",
      "|319-50-3348|B     |Mandalay |Normal       |female|Home and lifestyle    |40.3      |2       |4.03   |84.63   |3/11/2019|15:30|Ewallet    |80.6  |4.761904762            |4.03        |4.4   |\n",
      "+-----------+------+---------+-------------+------+----------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Da das Dateiformat nicht gepasst hat, wurde die Datei in LibreOffice geöffnet, und im Anschluss als CSV Datei gespeichert, um hier eingelesen werden zu können\n",
    "\n",
    "file = spark.read.csv('supermarket_sales.csv', header= True)\n",
    "file.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "722cbe6e-98da-4658-8205-45f88b00c6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|     m|    4|\n",
      "|     f|    3|\n",
      "|Female|  110|\n",
      "|female|    1|\n",
      "|  male|    1|\n",
      "|  Male|  136|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|     m|  141|\n",
      "|     f|  114|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) wie viele Einkäufe wurden von Frauen, bzw von Männern getätigt?\n",
    "\n",
    "# pyspark.sql.functions importieren für regexp_replace Funktion\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Wir schauen erstmal, wie viele verschiedene \"Arten\" von GenderTypen es gibt\n",
    "\n",
    "anzahlGender = file.groupBy('Gender').count()\n",
    "anzahlGender.show()\n",
    "\n",
    "# Es gibt jeweils für Männer und Frauen 3 Typen : m, male, Male / f, female, Female\n",
    "# Jetzt könnte man die Werte in den Spalten über RegEx abänder, sodass nur noch m/f drin vorkommen würde\n",
    "\n",
    "updategender1 = file.withColumn(\"Gender\", F.regexp_replace(F.col(\"Gender\"), \"male\", \"m\"))\n",
    "updategender2 = updategender1.withColumn(\"Gender\", F.regexp_replace(F.col(\"Gender\"), \"Male\", \"m\"))\n",
    "updategender3 = updategender2.withColumn(\"Gender\", F.regexp_replace(F.col(\"Gender\"), \"fem\", \"f\"))\n",
    "updategender4 = updategender3.withColumn(\"Gender\", F.regexp_replace(F.col(\"Gender\"), \"Fem\", \"f\"))\n",
    "\n",
    "anzahlGender = updategender4.groupBy('Gender').count()\n",
    "anzahlGender.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "699a7313-560e-4063-a274-03c67616b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Branch   |sum(Total)          |\n",
      "+---------+--------------------+\n",
      "|Mandalay |2.02011719595E7     |\n",
      "|Naypyitaw|1.6445864925000004E7|\n",
      "|Yangon   |1.5382536666000003E7|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) wie viel Umsatz wurde an den 3 Standorten jeweils getätigt - gib diesen zusammen mit dem NAMEN des jeweiligen Standorts aus\n",
    "# Nach City gruppieren und dazu noch die Summe aggregaten die als Umsatz getätigt wurden\n",
    "aggregationpipeline = {'Total' : 'sum'}\n",
    "standorte = file.groupBy('Branch').agg(aggregationpipeline)\n",
    "\n",
    "updateStandort1 = standorte.withColumn(\"Branch\", F.regexp_replace(F.col(\"Branch\"), \"A\", \"Yangon\"))\n",
    "updateStandort2 = updateStandort1.withColumn(\"Branch\", F.regexp_replace(F.col(\"Branch\"), \"B\", \"Mandalay\"))\n",
    "updateStandort3 = updateStandort2.withColumn(\"Branch\", F.regexp_replace(F.col(\"Branch\"), \"C\", \"Naypyitaw\"))\n",
    "\n",
    "updateStandort3.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1cbda7fa-2c89-400d-853d-8d3b3df20c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|    Payment|count|\n",
      "+-----------+-----+\n",
      "| creditcard|    1|\n",
      "|       cash|    2|\n",
      "| Creditcard|    3|\n",
      "|    ewallet|    2|\n",
      "|    Ewallet|   88|\n",
      "|       Cash|   82|\n",
      "|credit card|    1|\n",
      "|Credit card|   76|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|    Payment|      avg(Rating)|\n",
      "+-----------+-----------------+\n",
      "|    Ewallet|6.795555555555555|\n",
      "|       cash|6.983333333333333|\n",
      "|Kreditkarte| 7.13333333333333|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) welche Bezahlart die höchste durchschnittliche User-Satisfaction hat\n",
    "\n",
    "userSatisfaction = file.groupBy('Payment').count()\n",
    "userSatisfaction.show()\n",
    "\n",
    "# Erstmal die Bezahltypen anpassen damit wir ordentlich gruppieren können\n",
    "\n",
    "updatepayment1 = file.withColumn(\"Payment\", F.regexp_replace(F.col(\"Payment\"), \"creditcard\", \"Kreditkarte\"))\n",
    "updatepayment2 = updatepayment1.withColumn(\"Payment\", F.regexp_replace(F.col(\"Payment\"), \"Creditcard\", \"Kreditkarte\"))\n",
    "updatepayment3 = updatepayment2.withColumn(\"Payment\", F.regexp_replace(F.col(\"Payment\"), \"credit card\", \"Kreditkarte\"))\n",
    "updatepayment4 = updatepayment3.withColumn(\"Payment\", F.regexp_replace(F.col(\"Payment\"), \"Credit card\", \"Kreditkarte\"))\n",
    "updatepayment5 = updatepayment4.withColumn(\"Payment\", F.regexp_replace(F.col(\"Payment\"), \"Cash\", \"cash\"))\n",
    "updatepayment6 = updatepayment5.withColumn(\"Payment\", F.regexp_replace(F.col(\"Payment\"), \"ewallet\", \"Ewallet\"))\n",
    "\n",
    "# Jetzt nach der Payment-Methode gruppieren und die durschnittliche Rating Bewertung aggregaten\n",
    "payment = updatepayment6.groupBy('Payment').agg({'Rating' : 'avg'})\n",
    "\n",
    "# Jetzt noch nach dem Rating sortieren\n",
    "result = payment.orderBy('avg(Rating)')\n",
    "result.show()\n",
    "\n",
    "# Als eine CSV-Datei exportieren\n",
    "result.coalesce(1).write.csv(\"resultdata.csv\", header = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335ef0f",
   "metadata": {},
   "source": [
    "Aufgabe 6: [20/60 Punkte] Ein regionales Unternehmen hat ein großes Investment durch eine Venture Capital Firma bekommen, und möchte jetzt international expandieren. \n",
    "\n",
    "Euer Auftrag ist es das Unternehmen zum Thema \"Datenbanken\" (die für die internationale Expansion benötig werden) zu beraten.\n",
    "\n",
    "Das bisherige Hauptprodukt des Unternehmens sind mit Schokolade überzogene Fruchtgummis. Im Zuge der Expansion möchte das Unternehmen aber auch gleichzeitig neue Produkte einführen, bzw an regionale Geschmäcker anpassen. Dadurch, dass die Expansion überall gleichzeitig erfolgen soll, sind am Anfang wenig Möglichkeiten direkt Kundenumfragen zu machen - statt dessen soll die Resonanz auf Marketingcampagnen und die Interaktion mit der jeweiligen Landeswebseite genutzt werden, um lokale \"Geschmäcker\" und damit das Potential für weitere Produkte zu erforschen.\n",
    "\n",
    "Der bisher einzige Vertriebskanal ist eine Online-Präsenz, zumindest anfänglich soll dies auch so bleiben, jedoch mit potentiell separaten Webseiten für verschiedene Länder/Regionen. Hierzu besteht aber noch keine fest definierte Strategie, der Unternehmer würde von Euch auch gerne wissen, was für Möglichkeiten, Vor- und Nachteile, und Risiken verschiedener Ansätze (aus Sicht der Nutzung von Datenbanken) sind.\n",
    "\n",
    "Erläutere **stichwortartig**:\n",
    "\n",
    "1) welche Technologien in welchem Bereich eingesetzt werden, und warum\n",
    "2) was für Strategien, bzw Tools Ihr einsetzen wollt um \n",
    "    - die schon vorhandenen Daten zu migrieren\n",
    "    - neue Daten mit den alten zu verbinden\n",
    "    - benötigte Daten zu erhalten\n",
    "3) welche Strategien sich anbieten für das Reporting, die Datenhaltung, Backups etc.\n",
    "4) was die Auswirkungen verschiedener Expansionsstrategien (z.B. Marketing, Website etc) auf die Nutzung/Nützlichkeit/Performance der Datenbank(en) wäre \n",
    "5) Ob es \"Bottlenecks\" oder mit (hohem) Risiko verbundene Datenbank-Komponenten gibt, und wie man damit in der Praxis umgehen soll\n",
    "\n",
    "(wirklich nur stichwortartig, keine Romane!)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce676a04-7576-49e1-a991-c2144befe5f8",
   "metadata": {},
   "source": [
    "1. Verwendung von Kafka für EventStreaming um Infos einzuholen Für Webseiten relationale Datenbanken Docker Einsetzen um physikalische Anbeschaffungen gering zu halten Umfragen durchführen mit Kafka Subscribern und Publishern, auch mit Redis möglich Einführung von Data Lakes um alle Informationen zentral zu speichern Einsatz von Spark für schnelle lese und schreibzugriffe in memory\n",
    "\n",
    "2. Eine Key-Value Datenbank, zum Beispiel Redis, um Infos aus den Brokern der Kafka Architektur zu migiren Einsatz von Brokern um Infos zu sammeln Einsatz von Kafka Clustern Redis um daten zu verbinden\n",
    "\n",
    "3. Verwendung einer Airflow Architketur um regelmäßige backups durchzuführen anhand von tasks und plänen Reporting mit Kafka publisch und subscribe zum Datensammeln und Austausch\n",
    "\n",
    "4. Viele Anwendungen wollen auf Datenbank zugreifen Hohe Nachfrage von verschiednen Anwendungen Performance wird langsamer Daten könnten verloren gehen Datenbank könnte überlastet werden Viele unterschiedliche Daten werden in datenbank geschrieben, was zu unnötigen Einträgen führen könnte\n",
    "\n",
    "5. Datensicherheit/Datenverlust immer ein großes Thema Airflow Jobs für regelmäßige Backups einführen\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
